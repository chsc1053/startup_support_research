{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Title & Text (Abstract)\n",
    "title = \"An Importance of Using Virtualization Technology in Cloud Computing\"\n",
    "text = \"The past few decades have seen many changing trends in distributed computing systems in the form of grid, cloud, utility and cluster. With the advent in technology, there has been an increase in the demand for deployment of a robust distributed network for maximizing the performance of such systems and minimizing the infrastructural cost. In this paper we have discussed various levels at which virtualization can be implemented for distributed computing which can contribute to the increased efficiency and performance of distributed computing.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>KeyBERT</b>\n",
    "\n",
    "KeyBERT uses BERT (Bidirectional Encoder Representations from Transformers) to perform keyword extraction and sentence embedding.\n",
    "\n",
    "The BERT model is a pre-trained neural network model that is able to generate contextualized word embeddings.\n",
    "\n",
    "Working:\n",
    "1. Tokenizing the input text into subwords.\n",
    "2. Generating contextualized embeddings for each subword using the BERT model.\n",
    "3. Applying unsupervised clustering algorithms to group similar words or keyphrases together.\n",
    "4. Selecting the top N keywords or keyphrases from the clusters based on their relevance to the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\python_3.11.1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword:  distributed computing , Score 0.6487\n",
      "Keyword:  virtualization implemented , Score 0.6468\n",
      "Keyword:  virtualization , Score 0.6334\n",
      "Keyword:  levels virtualization , Score 0.5964\n",
      "Keyword:  grid cloud , Score 0.5326\n"
     ]
    }
   ],
   "source": [
    "# pip install keybert\n",
    "from keybert import KeyBERT\n",
    "\n",
    "model_keybert = KeyBERT(model='all-mpnet-base-v2')\n",
    "\n",
    "keywords = model_keybert.extract_keywords(text,keyphrase_ngram_range=(1, 2), stop_words='english', highlight=False, top_n=5)\n",
    "\n",
    "for kw, s in keywords:\n",
    "  print(\"Keyword: \",kw, \", Score\", s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>YAKE</b>\n",
    "\n",
    "YAKE (Yet Another Keyword Extractor) uses a statistical approach based on co-occurrence analysis and statistical significance testing.\n",
    "\n",
    "Working:\n",
    "\n",
    "1. The input text is pre-processed by removing stop words, punctuation, and other non-alphanumeric characters.\n",
    "\n",
    "2. YAKE uses co-occurrence analysis to identify frequent combinations of words in the text. This involves counting the number of times each word appears in the text and the number of times each pair of words co-occurs in the text. The co-occurrence matrix is then used to calculate a score for each combination of words.\n",
    "\n",
    "3. YAKE uses the Yule-Simpson coefficient to calculate the statistical significance of the co-occurrence of each pair of words. This coefficient measures the diversity of a word distribution and provides a measure of the statistical significance of a co-occurrence.\n",
    "\n",
    "4. Once the scores for all combinations of words have been calculated, YAKE selects the top N keywords or keyphrases based on their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword:  utility and cluster , Score 0.015583732582705545\n",
      "Keyword:  form of grid , Score 0.0191621476358869\n",
      "Keyword:  distributed computing systems , Score 0.020036360637023316\n",
      "Keyword:  past few decades , Score 0.02358133565825394\n",
      "Keyword:  changing trends , Score 0.02358133565825394\n"
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/LIAAD/yake\n",
    "import yake\n",
    "\n",
    "model_yake = yake.KeywordExtractor(top=5, stopwords=None)\n",
    "keywords = model_yake.extract_keywords(text)\n",
    "for kw, s in keywords:\n",
    "  print(\"Keyword: \",kw, \", Score\", s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PKE</b>\n",
    "\n",
    "PKE (Python Keyphrase Extraction) uses a statistical approach based on graph-based models to extract the most relevant keyphrases from a given text.\n",
    "\n",
    "Working:\n",
    "\n",
    "1. The input text is loaded into the PKE model using the load_document method and basic pre-processing such as lowercasing and removing non-alphanumeric characters are performed.\n",
    "\n",
    "2. The candidate_selection method is used to select the candidate keyphrases from the text. This method uses Part-Of-Speech (POS) tagging to identify the potential noun phrases and extracts them as candidate keyphrases.\n",
    "\n",
    "3. The candidate_weighting method is used to calculate the weights of the candidate keyphrases. The weights are calculated using a ranking algorithm that assigns scores to the candidate keyphrases based on their importance in the text.\n",
    "\n",
    "4. The get_n_best method is used to select the top N keyphrases from the candidate keyphrases based on their scores. The method returns a list of tuples, where each tuple contains a keyphrase and its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword:  computing systems , Score 0.08890515296435779\n",
      "Keyword:  performance , Score 0.07241923429281419\n",
      "Keyword:  cloud , Score 0.061749581996090555\n",
      "Keyword:  utility , Score 0.061228499575187195\n",
      "Keyword:  grid , Score 0.060025590579581026\n"
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/boudinfl/pke.git\n",
    "import pke\n",
    "\n",
    "model_pke = pke.unsupervised.TopicRank()\n",
    "\n",
    "model_pke.load_document(input=text, language='en')\n",
    "\n",
    "model_pke.candidate_selection()\n",
    "\n",
    "model_pke.candidate_weighting()\n",
    "\n",
    "keywords = model_pke.get_n_best(n=5)\n",
    "for kw, s in keywords:\n",
    "  print(\"Keyword: \",kw, \", Score\", s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Davinci</b>\n",
    "\n",
    "Davinci is a LLM (Large Language Model) developed by OpenAI that uses deep learning techniques to perform various natural language processing tasks such as language translation, text summarization, and keyword extraction.\n",
    "\n",
    "Davinci uses a transformer-based architecture that consists of a multi-layered neural network.\n",
    "\n",
    "Davinci is more advanced in keyword identification from text is that it has been pre-trained on massive amounts of textual data, enabling it to learn and recognize patterns and structures in language that are difficult for traditional rule-based algorithms to identify. This means that Davinci can understand the context and meaning of words in a sentence and identify important keywords and phrases more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Keywords: Virtualization, Cloud Computing, Distributed Computing\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"You will be given the title and the abstract of a paper. Please find out the appropriate keywords by analysing the abstract and relating them to the title.\\nTitle:\"+title+\"\\nAbstract: \"+text+\"\\n\\nRestrict the search space to technical keywords and give only 2-3 most important ones\"\n",
    ")\n",
    "print(response.choices[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
