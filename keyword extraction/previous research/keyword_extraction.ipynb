{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Title & Text (Abstract)\n",
    "text = \"Module 1 Introduction: What Is Machine Learning?, How Do We Define Learning?, How Do We Evaluate Our Networks?, How Do We Learn Our Network?, What are datasets and how to handle them?, Feature sets, Dataset division: test, train and validation sets, cross validation. Module 2 Basics of machine learning: Applications of Machine Learning, processes involved in Machine Learning, Introduction to Machine Learning Techniques: Supervised Learning, Unsupervised Learning and Reinforcement Learning, Real life examples of Machine Learning. Module 3 Supervised learning: Classification and Regression: K-Nearest Neighbor, Linear Regression, Logistic Regression, Support Vector Machine (SVM), Evaluation Measures: SSE, MME, R2, confusion matrix, precision, recall, F-Score, ROC-Curve. Module 4 Unsupervised learning: Introduction to clustering, Types of Clustering: Hierarchical, Agglomerative Clustering and Divisive clustering; Partitional Clustering - K-means clustering. Module 5 Miscellaneous: Dimensionality reduction techniques: PCA, LDA, ICA. Introduction to Deep Learning, Gaussian Mixture Models, Natural Language Processing, Computer Vision.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>KeyBERT</b>\n",
    "\n",
    "KeyBERT uses BERT (Bidirectional Encoder Representations from Transformers) to perform keyword extraction and sentence embedding.\n",
    "\n",
    "The BERT model is a pre-trained neural network model that is able to generate contextualized word embeddings.\n",
    "\n",
    "Working:\n",
    "1. Tokenizing the input text into subwords.\n",
    "2. Generating contextualized embeddings for each subword using the BERT model.\n",
    "3. Applying unsupervised clustering algorithms to group similar words or keyphrases together.\n",
    "4. Selecting the top N keywords or keyphrases from the clusters based on their relevance to the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction clustering\n",
      "machine learning\n",
      "unsupervised learning\n",
      "learning classification\n",
      "supervised learning\n",
      "clustering\n",
      "svm\n",
      "support vector\n",
      "means clustering\n",
      "machine svm\n"
     ]
    }
   ],
   "source": [
    "# pip install keybert\n",
    "from keybert import KeyBERT\n",
    "\n",
    "model_keybert = KeyBERT(model='all-mpnet-base-v2')\n",
    "\n",
    "keywords = model_keybert.extract_keywords(text,keyphrase_ngram_range=(1, 2), stop_words='english', highlight=False, top_n=10)\n",
    "\n",
    "for kw, s in keywords:\n",
    "  print(kw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>YAKE</b>\n",
    "\n",
    "YAKE (Yet Another Keyword Extractor) uses a statistical approach based on co-occurrence analysis and statistical significance testing.\n",
    "\n",
    "Working:\n",
    "\n",
    "1. The input text is pre-processed by removing stop words, punctuation, and other non-alphanumeric characters.\n",
    "\n",
    "2. YAKE uses co-occurrence analysis to identify frequent combinations of words in the text. This involves counting the number of times each word appears in the text and the number of times each pair of words co-occurs in the text. The co-occurrence matrix is then used to calculate a score for each combination of words.\n",
    "\n",
    "3. YAKE uses the Yule-Simpson coefficient to calculate the statistical significance of the co-occurrence of each pair of words. This coefficient measures the diversity of a word distribution and provides a measure of the statistical significance of a co-occurrence.\n",
    "\n",
    "4. Once the scores for all combinations of words have been calculated, YAKE selects the top N keywords or keyphrases based on their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Our Networks\n",
      "Learn Our Network\n",
      "Machine Learning\n",
      "Machine Learning Techniques\n",
      "Feature sets\n",
      "Support Vector Machine\n",
      "Define Learning\n",
      "Learning\n",
      "Machine\n",
      "Supervised Learning\n"
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/LIAAD/yake\n",
    "import yake\n",
    "\n",
    "model_yake = yake.KeywordExtractor(top=10, stopwords=None)\n",
    "keywords = model_yake.extract_keywords(text)\n",
    "for kw, s in keywords:\n",
    "  print(kw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PKE</b>\n",
    "\n",
    "PKE (Python Keyphrase Extraction) uses a statistical approach based on graph-based models to extract the most relevant keyphrases from a given text.\n",
    "\n",
    "Working:\n",
    "\n",
    "1. The input text is loaded into the PKE model using the load_document method and basic pre-processing such as lowercasing and removing non-alphanumeric characters are performed.\n",
    "\n",
    "2. The candidate_selection method is used to select the candidate keyphrases from the text. This method uses Part-Of-Speech (POS) tagging to identify the potential noun phrases and extracts them as candidate keyphrases.\n",
    "\n",
    "3. The candidate_weighting method is used to calculate the weights of the candidate keyphrases. The weights are calculated using a ranking algorithm that assigns scores to the candidate keyphrases based on their importance in the text.\n",
    "\n",
    "4. The get_n_best method is used to select the top N keyphrases from the candidate keyphrases based on their scores. The method returns a list of tuples, where each tuple contains a keyphrase and its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n",
      "clustering\n",
      "module\n",
      "introduction\n",
      "regression\n",
      "unsupervised learning\n",
      "validation sets\n",
      "processes\n",
      "datasets\n",
      "dimensionality reduction techniques\n"
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/boudinfl/pke.git\n",
    "import pke\n",
    "\n",
    "model_pke = pke.unsupervised.TopicRank()\n",
    "\n",
    "model_pke.load_document(input=text, language='en')\n",
    "\n",
    "model_pke.candidate_selection()\n",
    "\n",
    "model_pke.candidate_weighting()\n",
    "\n",
    "keywords = model_pke.get_n_best(n=10)\n",
    "for kw, s in keywords:\n",
    "  print(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>GPT</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n",
      "Definition of Learning\n",
      "Evaluation of Networks\n",
      "Handling Datasets\n",
      "Feature Sets\n",
      "Dataset Division\n",
      "Cross Validation\n",
      "Supervised Learning\n",
      "Unsupervised Learning\n",
      "Reinforcement Learning\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"\"\n",
    "system_intel = \"You will be given data containing the syllabus of a computer science related program. Your task is to extract the topics from the syllabus as keywords. The keywords must be non-repetitive and computer science related, DO NOT include general terms. YOUR RESPONSE THROUGH API WILL BE USED IN AN APPLICATION DIRECTLY. Hence, STRICTLY FOLLOW THE FORMAT. DO NOT GENERATE ANY EXTRA TEXT OTHER THAN THE KEYWORDS SEPARATED BY COMMAS. STRICTLY GENERATE ONLY 10 KEYWORDS.\"\n",
    "\n",
    "result = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_intel},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    ")\n",
    "response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "keywords = response.split(\", \")\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Davinci</b>\n",
    "\n",
    "Davinci is a LLM (Large Language Model) developed by OpenAI that uses deep learning techniques to perform various natural language processing tasks such as language translation, text summarization, and keyword extraction.\n",
    "\n",
    "Davinci uses a transformer-based architecture that consists of a multi-layered neural network.\n",
    "\n",
    "Davinci is more advanced in keyword identification from text is that it has been pre-trained on massive amounts of textual data, enabling it to learn and recognize patterns and structures in language that are difficult for traditional rule-based algorithms to identify. This means that Davinci can understand the context and meaning of words in a sentence and identify important keywords and phrases more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# openai.api_key = \"\"\n",
    "\n",
    "# response = openai.Completion.create(\n",
    "#   model=\"text-davinci-003\",\n",
    "#   prompt=\"You will be given the title and the abstract of a paper. Please find out the appropriate keywords by analysing the abstract and relating them to the title.\\nTitle:\"+title+\"\\nAbstract: \"+text+\"\\n\\nRestrict the search space to technical keywords and give only 2-3 most important ones\"\n",
    "# )\n",
    "# print(response.choices[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
